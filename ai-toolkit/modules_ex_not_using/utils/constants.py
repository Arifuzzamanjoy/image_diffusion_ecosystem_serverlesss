"""
Constants for the Advanced FLUX LoRA Trainer
"""

# Dataset limits
MAX_IMAGES = 150

# Supported file extensions
SUPPORTED_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.webp', '.bmp'}

# Default configurations
DEFAULT_CONFIG_YAML = '''# Advanced training configuration
device: cuda:0
model:
  is_flux: true
  quantize: true
network:
  linear: 16
  linear_alpha: 16
  type: lora
sample:
  guidance_scale: 3.5
  height: 1024
  neg: ''
  sample_every: 1000
  sample_steps: 28
  sampler: flowmatch
  seed: 42
  walk_seed: true
  width: 1024
save:
  dtype: float16
  hf_private: true
  max_step_saves_to_keep: 4
  push_to_hub: true
  save_every: 250
train:
  batch_size: 1
  dtype: bf16
  ema_config:
    ema_decay: 0.99
    use_ema: true
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  noise_scheduler: flowmatch
  optimizer: adamw8bit
  train_text_encoder: false
  train_unet: true
  linear_timesteps: false
  skip_first_sample: true
'''

# Model configurations
FLUX_MODELS = {
    'dev': {
        'name_or_path': 'black-forest-labs/FLUX.1-dev',
        'assistant_lora_path': None,
        'guidance_scale': 3.5,
        'sample_steps': 28
    },
    'schnell': {
        'name_or_path': 'black-forest-labs/FLUX.1-schnell',
        'assistant_lora_path': 'ostris/FLUX.1-schnell-training-adapter',
        'guidance_scale': 1.0,
        'sample_steps': 4
    }
}

# Optimizer configurations
OPTIMIZERS = {
    'adamw': 'Standard AdamW optimizer',
    'adamw8bit': '8-bit AdamW (memory efficient)',
    'lion': 'Lion optimizer (faster convergence)',
    'adafactor': 'Adafactor (very memory efficient)'
}

# Noise schedulers
NOISE_SCHEDULERS = {
    'flowmatch': 'Flow Matching (FLUX default)',
    'ddpm': 'DDPM scheduler',
    'dpm': 'DPM scheduler',
    'euler': 'Euler scheduler'
}

# Training precisions
TRAINING_DTYPES = {
    'bf16': 'BFloat16 (recommended)',
    'fp16': 'Float16 (faster, less stable)',
    'fp32': 'Float32 (slowest, most stable)'
}

# Save precisions
SAVE_DTYPES = {
    'float16': 'Float16 (smaller files)',
    'float32': 'Float32 (higher precision)',
    'bf16': 'BFloat16 (best compatibility)'
}

# Resolution options
RESOLUTION_OPTIONS = [512, 768, 1024, 1280, 1536]

# Default sample prompts
DEFAULT_SAMPLE_PROMPTS = [
    "A photo of {trigger} in a beautiful landscape",
    "Portrait of {trigger}, high quality",
    "{trigger} in an artistic style"
]

# UI Messages
UI_MESSAGES = {
    'dataset_upload_help': """
Upload both files to continue:
1. **Images ZIP**: Contains your training images (numbered 0001.png, 0002.png, etc.)
2. **Captions JSON**: Generated by Advanced Image Captioning Pro
    """,
    
    'training_help': """
Configure your training parameters:
- **LoRA Name**: Unique identifier for your model
- **Trigger Word**: Word/phrase to activate your LoRA
- **Steps**: Number of training iterations (1000 recommended)
- **Learning Rate**: How fast the model learns (1e-4 recommended)
    """,
    
    'advanced_help': """
Advanced settings for experienced users:
- **Optimizer**: Training algorithm (adamw8bit recommended)
- **Precision**: Memory vs quality tradeoff
- **Quantization**: Reduces memory usage
- **EMA**: Smooths training for better quality
    """
}

# Error messages
ERROR_MESSAGES = {
    'no_dataset': "Please load dataset first by uploading images ZIP and captions JSON.",
    'no_lora_name': "Please provide a LoRA name! This name must be unique.",
    'invalid_zip': "Invalid ZIP file. Please ensure it contains numbered image files.",
    'invalid_json': "Invalid JSON file. Please ensure it's from Advanced Image Captioning Pro.",
    'insufficient_images': "Dataset must contain at least 2 matched images for training.",
    'too_many_images': f"Too many images! Maximum {MAX_IMAGES} allowed.",
    'cuda_error': "CUDA error encountered. Try enabling Low VRAM mode or reducing batch size.",
    'memory_error': "Out of memory. Try reducing batch size, enabling quantization, or using Low VRAM mode."
}

# Success messages
SUCCESS_MESSAGES = {
    'dataset_loaded': "âœ… Dataset successfully loaded and verified!",
    'training_complete': "ðŸŽ‰ Training completed successfully!",
    'model_saved': "ðŸ’¾ Model saved successfully!"
}

# File paths
PATHS = {
    'config_template': "../ai-toolkit/config/examples/train_lora_flux_24gb.yaml",
    'tmp_dir': "../tmp",
    'datasets_dir': "../datasets",
    'output_dir': "../output"
}